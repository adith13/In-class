{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63142152-2a31-4e4f-832f-f8d7cb245a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6fbd9f-ccdb-4ab3-93c7-30f274baabfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue:  book_grammars,brown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
      "  [ ] omw................. Open Multilingual Wordnet\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet2021......... Open English Wordnet 2021\n",
      "  [ ] wordnet2022......... Open English Wordnet 2022\n",
      "  [ ] wordnet31........... Wordnet 3.1\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [P] all-corpora......... All the corpora\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  book, inaugural, brown, webtext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Error loading book,: Package 'book,' not found in index\n",
      "    Error loading inaugural,: Package 'inaugural,' not found in index\n",
      "    Error loading brown,: Package 'brown,' not found in index\n",
      "    Downloading package webtext to\n",
      "        C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "      Unzipping corpora\\webtext.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  book\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading collection 'book'\n",
      "       | \n",
      "       | Downloading package abc to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\abc.zip.\n",
      "       | Downloading package brown to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\brown.zip.\n",
      "       | Downloading package chat80 to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\chat80.zip.\n",
      "       | Downloading package cmudict to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\cmudict.zip.\n",
      "       | Downloading package conll2000 to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\conll2000.zip.\n",
      "       | Downloading package conll2002 to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\conll2002.zip.\n",
      "       | Downloading package dependency_treebank to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\dependency_treebank.zip.\n",
      "       | Downloading package genesis to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\genesis.zip.\n",
      "       | Downloading package gutenberg to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\gutenberg.zip.\n",
      "       | Downloading package ieer to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\ieer.zip.\n",
      "       | Downloading package inaugural to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\inaugural.zip.\n",
      "       | Downloading package movie_reviews to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\movie_reviews.zip.\n",
      "       | Downloading package nps_chat to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\nps_chat.zip.\n",
      "       | Downloading package names to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\names.zip.\n",
      "       | Downloading package ppattach to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\ppattach.zip.\n",
      "       | Downloading package reuters to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       | Downloading package senseval to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\senseval.zip.\n",
      "       | Downloading package state_union to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\state_union.zip.\n",
      "       | Downloading package stopwords to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Package stopwords is already up-to-date!\n",
      "       | Downloading package swadesh to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\swadesh.zip.\n",
      "       | Downloading package timit to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\timit.zip.\n",
      "       | Downloading package treebank to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\treebank.zip.\n",
      "       | Downloading package toolbox to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\toolbox.zip.\n",
      "       | Downloading package udhr to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\udhr.zip.\n",
      "       | Downloading package udhr2 to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\udhr2.zip.\n",
      "       | Downloading package unicode_samples to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\unicode_samples.zip.\n",
      "       | Downloading package webtext to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Package webtext is already up-to-date!\n",
      "       | Downloading package wordnet to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       | Downloading package wordnet_ic to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\wordnet_ic.zip.\n",
      "       | Downloading package words to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\words.zip.\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "       | Downloading package maxent_ne_chunker to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "       | Downloading package universal_tagset to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping taggers\\universal_tagset.zip.\n",
      "       | Downloading package punkt to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Package punkt is already up-to-date!\n",
      "       | Downloading package book_grammars to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping grammars\\book_grammars.zip.\n",
      "       | Downloading package city_database to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping corpora\\city_database.zip.\n",
      "       | Downloading package tagsets to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping help\\tagsets.zip.\n",
      "       | Downloading package panlex_swadesh to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "       |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "       | \n",
      "     Done downloading collection book\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  brown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command 'brown' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  inaugural\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command 'inaugural' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  brown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package brown to\n",
      "        C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "      Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  inaugural\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package inaugural to\n",
      "        C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "      Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package stopwords to\n",
      "        C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "      Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  web_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Error loading web_text: Package 'web_text' not found in index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d933b3ef-3fd0-4f5a-9c3e-84f50141f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e97e6f4-c9ab-4666-85df-e407c4f7df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40677ee5-b48e-465b-b87d-4278756a4bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb785020-7b87-4788-8a56-701d575ad565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan',\n",
       " 'Morgan',\n",
       " 'told',\n",
       " 'himself',\n",
       " 'he',\n",
       " 'would',\n",
       " 'forget',\n",
       " 'Ann',\n",
       " 'Turner',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'well',\n",
       " 'rid',\n",
       " 'of',\n",
       " 'her',\n",
       " '.',\n",
       " 'He',\n",
       " 'certainly',\n",
       " \"didn't\",\n",
       " 'want',\n",
       " 'a',\n",
       " 'wife',\n",
       " 'who',\n",
       " 'was',\n",
       " 'fickle',\n",
       " 'as',\n",
       " 'Ann',\n",
       " '.',\n",
       " 'If',\n",
       " 'he',\n",
       " 'had',\n",
       " 'married',\n",
       " 'her',\n",
       " ',',\n",
       " \"he'd\",\n",
       " 'have',\n",
       " 'been',\n",
       " 'asking',\n",
       " 'for',\n",
       " 'trouble',\n",
       " '.',\n",
       " 'But',\n",
       " 'all',\n",
       " 'of',\n",
       " 'this',\n",
       " 'was',\n",
       " 'rationalization',\n",
       " '.',\n",
       " 'Sometimes']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories = \"adventure\")[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f608496-8be0-4134-b73d-ebca6e7c2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761dee8a-ee20-4fcd-8a6a-6b38ace25232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt',\n",
       " '2021-Biden.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e87276-b128-4afd-b5d3-995f59d11b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow',\n",
       " '-',\n",
       " 'Citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " ':',\n",
       " 'In',\n",
       " 'compliance',\n",
       " 'with',\n",
       " 'a',\n",
       " 'custom',\n",
       " 'as',\n",
       " 'old',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Government',\n",
       " 'itself',\n",
       " ',']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.words(fileids = '1861-Lincoln.txt')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fa9dd-a848-498f-b3b7-2878fa97f03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
